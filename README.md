## Basic_Web_Crawler

 A. in this repository, I will demontrate basic web crawler knowledge

 B. there are packages about urllib, cookies and mock_login 
 
 ---
 
 1. real_url : basic way to read a url , and output response, data , and write file: google.html

 2. get_params : add header to prevent from web-blocking and parse url when deal wuth chinese words

 3. get_params_2 : facing dict_params, we use urllib.parse.urlencode(parmas) to parse them to url string

 4. request_header : how to add headers and get full url, response and request headers 

 5. random_user_agent : how to add multiple user-agent and choose randomly everytime.
 
 6. handler_opener : use handler and opener to get data



