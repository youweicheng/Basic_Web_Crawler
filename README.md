## Basic_Web_Crawler

 A. in this repository, I will demontrate basic web crawler knowledge

 B. there are packages about urllib, cookies and mock_login 
 
 ---
 
 1. real_url : basic way to read a url , and output response, data , and write file: google.html

 2. get_params : add header to prevent from web-blocking and parse url when deal wuth chinese words

 3. get_params_2 : facing dict_params, we use urllib.parse.urlencode(parmas) to parse them to url string

 4. request_header : how to add headers and get full url, response and request headers 

 5. random_user_agent : how to add multiple user-agent and choose randomly everytime.
 
 6. handler_opener : use handler and opener to get data

 7. proxy_handler : use proxy ip to get data

 8. multi_proxy : create multiple proxy and get data

 9. paid_proxy_handler : two ways to add paid proxy ip

 10. auth_use : use for Intranet

 11. cookies_1 : quick review for adding cookies

 12. cookies_2 : add cookies by hand

 13. cookies_3 : use cookiejar to add cookies automatically

 14. Errors : urllib.request Errors : HTTPError / URLError

 15. requests_use : introduction of requests package

 16. requests_use2 : get info from requests package

 17. requests_use3 : use requests with params

 18. requests_use4 : use json function to turn str into dict

 19. requests_auth : post request and auth request

 20. requests_proxy : use free proxy on requests package

 21. requests_ssl : deal with requests.exceptions.SSLError issue

 22. requests_cookies : add cookies on requests package
 
 23. requests_cookies2 : use session to add cookies
